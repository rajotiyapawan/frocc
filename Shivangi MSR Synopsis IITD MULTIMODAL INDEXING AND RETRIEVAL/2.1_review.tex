\section{A Systematic Review of Cross-modal Retrieval Methods}

Humans interact with each other using a significant amount of multimodal data on a daily basis. We click images, write long texts, and record short videos all the time. With emerging social media and entertainment, users' expectations for search results in different modalities are increasing. As a result, the cross-modal retrieval task has experienced increased interest in the research community in recent years. Cross-modal retrieval can be defined as querying an interactive system using one modality, such as text, and retrieving relevant results in another modality, such as images, from a multimodal database. Thanks to the prosperity of attention-based transformer architectures like BERT(Bidirectional Encoder Representations from Transformers), GPT(Generative Pre-trained Transformers) in NLP, and ViT(Vision Transformers) in CV, we have witnessed the great success of the pre-training paradigm in past few years. Pre-training large models have replaced the classical methods like LDA, Word2Vec, TF-IDF, Histogram of gradients, SIFT, and Faster-RCNN for feature representation. This shift to transformer-based architecture and self-supervised learning in NLP and Vision has inspired the multimodal literature to adopt similar trends for learning joint embeddings of image and textual data.

It is important to mention that many previous efforts have been dedicated to conduct a survey on cross-modal retrieval, but they either focus on multi-modal learning \cite{multi}. or consider many multimodal tasks \cite{xu2023multimodal} like image-captioning \cite{vinyals2015tell}, visual question answering \cite{VQA}, recommendation systems \cite{recsys}, etc., or do not include many important works proposed in recent years \cite{cao}. Therefore, this paper aims to conduct a comprehensive survey of vision-language pre-trained models, also known as foundational models, from a perspective of image-text retrieval \cite{flickr8k}. These models learn the image-text alignment using self-supervised pre-training tasks. We are not considering any supervised or unsupervised learning methodologies in this work. We systematically searched Google Scholar and Semantic Scholar with relevant keywords like "vision-language", "cross-modal pre-train", "image-text pre-train", "multi-modal pre-train", etc. We performed manual filtering to collect 15 relevant papers for the survey.

Vision-Language Pre-Trained (VLPT) models learn multimodal data representation from large amounts of image-text pairs, which can be noisy image-text data crawled from the web or annotated large-scale datasets like Visual Genome \cite{krishna2016visual}, SBU Captions \cite{im2text}, Conceptual Captions \cite{conceptualcaptions}, and MSCOCO \cite{Lin2014MicrosoftCC}. A VLPT model generally contains a data augmentation process, a feature representation process, and some pre-training paradigm for self-supervised learning. This pre-trained model is then fine-tuned for downstream tasks like image-text retrieval \cite{flickr8k}. We construct a taxonomy from three perspectives to overview cross-modal image-text retrieval approaches. 

\begin{enumerate}
    \item \textbf{Data Augmentation:} Existing VLPT models learn robust multimodal feature representations by applying various data augmentation techniques to raw image-text data. These techniques fall into three categories: text augmentation techniques, image augmentation techniques, and joint multi-modal augmentation techniques. 
    
    \item \textbf{Data Pre-processing:} Humans interpret raw image-text data differently than computers, resulting in a semantic gap. Data pre-processing is critical in bridging this semantic gap by converting raw image and text data into a format VLPT models can effectively process. Existing VLPT models employ a variety of data pre-processing steps divided into two broad categories: text pre-processing techniques and image pre-processing techniques. 

    \item \textbf{Pre-training Paradigm:} VLPT models yield stronger performance than conventional multi-modal models on cross-modal retrieval task. To better understand the reason behind this encouraging performance gain, we categorized the pre-training paradigm into three categories: pre-training data, model architecture, and pre-training tasks. 
\end{enumerate}

\subsection{Key Contributions}
\begin{enumerate}
    \item To the best of our knowledge, this is the first comprehensive survey of large pre-trained models, also known as foundational models, from a cross-modal retrieval perspective. As most large-scale pre-trained architectures are only designed for image and text modalities, we focused on vision-language models in this survey and compared them for image and text retrieval tasks.
    \item We summarized various data augmentation, pre-processing, and pre-training techniques used to learn effective multi-modal representations by VLPT models to solve cross-modal retrieval task.
    \item We also identified shortcomings and challenges in current VLPT models and discussed potential future work directions.
\end{enumerate}

\subsection{Future Work}
\begin{enumerate}
    \item \textbf{Large-scale multi-modal datasets: } Due to a lack of large-scale aligned multimodal data for modalities such as audio, video, table, time series, and so on, only a few studies have considered training a multi-modal pre-trained model. Therefore, large-scale datasets containing various multimodal signals can be collected.
    \item \textbf{Modality Independent Pre-training Architectures:} So far, the development of multi-modal pre-trained models requires aligned multimodal data, as current architectures only exploit inter-modal relations for representation learning. Previous modality-independent models employed effective label space learning, which can be adapted for current VLPT architectures to develop modality-independent pre-trained multi-modal models.
    \item \textbf{Sustainability:} Large-scale models with billions of trainable parameters necessitate substantial computational resources, pre-training data, memory, and training time. They also harm the environment by consuming large amounts of electricity and water. Thus, developing such large-scale models sustainably in the future is critical.
    \item \textbf{Efficient Multi-modal Indexing:} Large latency and enormous computational cost, primarily due to cross-modal attention in transformer architecture, prevent these VLPT models from being used in real-world applications. Few works have addressed this issue using efficient data indexing and dual encoder techniques. More efforts are required in the future to develop practical and deployable VLPT models.
    \item \textbf{Evaluation Metric:} In the VLPT literature, Recall@K \{K = 1, 5 and 10\} is the most commonly used evaluation metric. Other metrics like mAP@R and nDCG@K, which are more aligned to humans, as observed by Chun et al. \cite{Chun2022ECCVCC}, are rarely considered for evaluation and thus can be explored in future works.
\end{enumerate}
