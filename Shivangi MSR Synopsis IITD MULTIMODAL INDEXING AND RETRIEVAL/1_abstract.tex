\centerline{ \large \bf Abstract}

The proliferation of multimodal data, which includes images, texts, audio, videos, and tables, has underscored the significance of addressing cross-modal retrieval and generation challenges. Cross-modal retrieval involves retrieving items from a modality distinct from the query, whereas cross-modal generation generates items from a modality distinct from the input modality. Current cross-modal retrieval approaches often involve projecting multimodal features into a shared space, facilitating direct comparison between items. However, existing methodologies exhibit limitations, particularly in the context of multi-label datasets, multi-view learning, scalability, and the naive selection of nearest neighbors in the common space. Current evaluation metrics for cross-modal generation entail using feature-based or n-gram-based matching techniques to compare generated and ground truth data. However, these heuristic-driven evaluation metrics lack both interpretability and usability. We propose two novel cross-modal retrieval frameworks in an effort to resolve the aforementioned concerns. To address the challenges associated with cross-modal retrieval methods, we learn a lightweight neural network model containing individual modality encoders and an auto-encoder network for the labels, which transforms the low-level input data embeddings to a common representation space and employs a novel 2-stage k-nearest neighbor (2Sknn) search algorithm to recommend relevant items to the query. We also propose its extension to multi-view learning beyond the image and text modalities. In light of the challenges associated with the systematic evaluation of generative models, we put forth a unified cross-modal retrieval framework that executes the retrieval task via generative augmentation. We conduct comprehensive experiments using publicly available benchmark datasets to provide valuable insights into the behaviour of the algorithm for both frameworks.

\vspace*{24pt}

\noindent KEYWORDS: \hspace*{0.5em} \parbox[t]{4.4in}{Multimodal machine learning, cross-modal retrieval, representation learning, evaluation method, cross-modal generative model}

\pagebreak