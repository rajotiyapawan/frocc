@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@inproceedings{wikitables,
  title={TabEL: Entity Linking in Web Tables},
  author={Chandra Bhagavatula and Thanapon Noraset and Doug Downey},
  booktitle={International Workshop on the Semantic Web},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14265783}
}

@inproceedings{svhn,
author = {Hu, Peng and Wang, Xu and Zhen, Liangli and Peng, Dezhong},
title = {Separated Variational Hashing Networks for Cross-Modal Retrieval},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351078},
doi = {10.1145/3343031.3351078},
abstract = {Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1721–1729},
numpages = {9},
keywords = {cross-modal hashing, separated variational hashing network, common hamming space, cross-modal retrieval},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{ocmfh,
author = {Wang, Di and Wang, Quan and An, Yaqiang and Gao, Xinbo and Tian, Yumin},
title = {Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401132},
doi = {10.1145/3397271.3401132},
abstract = {Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1409–1418},
numpages = {10},
keywords = {large-scale retrieval, matrix factorization, online learning, cross-modal hashing},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{mirflickr25k,
author = {Huiskes, Mark J. and Lew, Michael S.},
title = {The MIR Flickr Retrieval Evaluation},
year = {2008},
isbn = {9781605583129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1460096.1460104},
doi = {10.1145/1460096.1460104},
abstract = {In most well known image retrieval test sets, the imagery typically cannot be freely distributed or is not representative of a large community of users. In this paper we present a collection for the MIR community comprising 25000 images from the Flickr website which are redistributable for research purposes and represent a real community of users both in the image content and image tags. We have extracted the tags and EXIF image metadata, and also make all of these publicly available. In addition we discuss several challenges for benchmarking retrieval and classification methods.},
booktitle = {Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval},
pages = {39–43},
numpages = {5},
keywords = {benchmarking, relevance feedback, content-based image retrieval, image collections},
location = {Vancouver, British Columbia, Canada},
series = {MIR '08}
}

@ARTICLE{srlch,  author={Shen, Heng Tao and Liu, Luchen and Yang, Yang and Xu, Xing and Huang, Zi and Shen, Fumin and Hong, Richang},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={Exploiting Subspace Relation in Semantic Labels for Cross-Modal Hashing},   year={2021},  volume={33},  number={10},  pages={3351-3365},  doi={10.1109/TKDE.2020.2970050}}

@inproceedings{cdpae,
author = {Zhan, Yibing and Yu, Jun and Yu, Zhou and Zhang, Rong and Tao, Dacheng and Tian, Qi},
title = {Comprehensive Distance-Preserving Autoencoders for Cross-Modal Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240607},
doi = {10.1145/3240508.3240607},
abstract = {In this paper, we propose a novel method with comprehensive distance-preserving autoencoders (CDPAE) to address the problem of unsupervised cross-modal retrieval. Previous unsupervised methods rely primarily on pairwise distances of representations extracted from cross media spaces that co-occur and belong to the same objects. However, besides pairwise distances, the CDPAE also considers heterogeneous distances of representations extracted from cross media spaces as well as homogeneous distances of representations extracted from single media spaces that belong to different objects. The CDPAE consists of four components. First, denoising autoencoders are used to retain the information from the representations and to reduce the negative influence of redundant noises. Second, a comprehensive distance-preserving common space is proposed to explore the correlations among different representations. This aims to preserve the respective distances between the representations within the common space so that they are consistent with the distances in their original media spaces. Third, a novel joint loss function is defined to simultaneously calculate the reconstruction loss of the denoising autoencoders and the correlation loss of the comprehensive distance-preserving common space. Finally, an unsupervised cross-modal similarity measurement is proposed to further improve the retrieval performance. This is carried out by calculating the marginal probability of two media objects based on a kNN classifier. The CDPAE is tested on four public datasets with two cross-modal retrieval tasks: "query images by texts" and "query texts by images". Compared with eight state-of-the-art cross-modal retrieval methods, the experimental results demonstrate that the CDPAE outperforms all the unsupervised methods and performs competitively with the supervised methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1137–1145},
numpages = {9},
keywords = {similarity measurement, comprehensive distancepreserving, autoencoder, cross-modal retrieval, unsupervised},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{sdml,
author = {Hu, Peng and Zhen, Liangli and Peng, Dezhong and Liu, Pei},
title = {Scalable Deep Multimodal Learning for Cross-Modal Retrieval},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331213},
doi = {10.1145/3331184.3331213},
abstract = {Cross-modal retrieval takes one type of data as the query to retrieve relevant data of another type. Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. For these approaches, the optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities. In this paper, we present a novel cross-modal retrieval method, called Scalable Deep Multimodal Learning (SDML). It proposes to predefine a common subspace, in which the between-class variation is maximized while the within-class variation is minimized. Then, it trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. Unlike many of the existing methods, our method can train different modality-specific networks independently and thus be scalable to the number of modalities. To the best of our knowledge, the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {635–644},
numpages = {10},
keywords = {multimodal learning, cross-modal retrieval, representation learning},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{acmr,
author = {Wang, Bokun and Yang, Yang and Xu, Xing and Hanjalic, Alan and Shen, Heng Tao},
title = {Adversarial Cross-Modal Retrieval},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123326},
doi = {10.1145/3123266.3123326},
abstract = {Cross-modal retrieval aims to enable flexible retrieval experience across different modalities (e.g., texts vs. images). The core of cross-modal retrieval research is to learn a common subspace where the items of different modalities can be directly compared to each other. In this paper, we present a novel Adversarial Cross-Modal Retrieval (ACMR) method, which seeks an effective common subspace based on adversarial learning. Adversarial learning is implemented as an interplay between two processes. The first process, a feature projector, tries to generate a modality-invariant representation in the common subspace and to confuse the other process, modality classifier, which tries to discriminate between different modalities based on the generated representation. We further impose triplet constraints on the feature projector in order to minimize the gap among the representations of all items from different modalities with same semantic labels, while maximizing the distances among semantically different images and texts. Through the joint exploitation of the above, the underlying cross-modal semantic structure of multimedia data is better preserved when this data is projected into the common subspace. Comprehensive experimental results on four widely used benchmark datasets show that the proposed ACMR method is superior in learning effective subspace representation and that it significantly outperforms the state-of-the-art cross-modal retrieval methods.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {154–162},
numpages = {9},
keywords = {modality gap, adversarial learning, cross-modal retrieval},
location = {Mountain View, California, USA},
series = {MM '17}
}

@misc{dsmhn,
      title={Deep Semantic Multimodal Hashing Network for Scalable Multimedia Retrieval}, 
      author={Zechao Li and Lu Jin and Jinhui Tang},
      year={2019},
      eprint={1901.02662},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{corr-ae,
author = {Feng, Fangxiang and Wang, Xiaojie and Li, Ruifan},
title = {Cross-Modal Retrieval with Correspondence Autoencoder},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654902},
doi = {10.1145/2647868.2654902},
abstract = {The problem of cross-modal retrieval, e.g., using a text query to search for images and vice-versa, is considered in this paper. A novel model involving correspondence autoencoder (Corr-AE) is proposed here for solving this problem. The model is constructed by correlating hidden representations of two uni-modal autoencoders. A novel optimal objective, which minimizes a linear combination of representation learning errors for each modality and correlation learning error between hidden representations of two modalities, is used to train the model as a whole. Minimization of correlation learning error forces the model to learn hidden representations with only common information in different modalities, while minimization of representation learning error makes hidden representations are good enough to reconstruct input of each modality. A parameter $alpha$ is used to balance the representation learning error and the correlation learning error. Based on two different multi-modal autoencoders, Corr-AE is extended to other two correspondence models, here we called Corr-Cross-AE and Corr-Full-AE. The proposed models are evaluated on three publicly available data sets from real scenes. We demonstrate that the three correspondence autoencoders perform significantly better than three canonical correlation analysis based models and two popular multi-modal deep models on cross-modal retrieval tasks.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {7–16},
numpages = {10},
keywords = {deep learning, image and text, retrieval, autoencoder, cross-modal},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@INPROCEEDINGS{dscmr,  author={Zhen, Liangli and Hu, Peng and Wang, Xu and Peng, Dezhong},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Supervised Cross-Modal Retrieval},   year={2019},  volume={},  number={},  pages={10386-10395},  doi={10.1109/CVPR.2019.01064}}

@article{daml,
author = {Xu, Xing and He, Li and Lu, Huimin and Gao, Lianli and Ji, Yanli},
title = {Deep Adversarial Metric Learning for Cross-Modal Retrieval},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0541-x},
doi = {10.1007/s11280-018-0541-x},
abstract = {Cross-modal retrieval has become a highlighted research topic, to provide flexible retrieval experience across multimedia data such as image, video, text and audio. The core of existing cross-modal retrieval approaches is to narrow down the gap between different modalities either by finding a maximally correlated embedding space. Recently, researchers leverage Deep Neural Network (DNN) to learn nonlinear transformations for each modality to obtain transformed features in a common subspace where cross-modal matching can be performed. However, the statistical characteristics of the original features for each modality are not explicitly preserved in the learned subspace. Inspired by recent advances in adversarial learning, we propose a novel Deep Adversarial Metric Learning approach, termed DAML for cross-modal retrieval. DAML nonlinearly maps labeled data pairs of different modalities into a shared latent feature subspace, under which the intra-class variation is minimized and the inter-class variation is maximized, and the difference of each data pair captured from two modalities of the same class is minimized, respectively. In addition to maximizing the correlations between modalities, we add an additional regularization by introducing adversarial learning. In particular, we introduce a modality classifier to predict the modality of a transformed feature, which ensures that the transformed features are also statistically indistinguishable. Experiments on three popular multimodal datasets show that DAML achieves superior performance compared to several state of the art cross-modal retrieval methods.},
journal = {World Wide Web},
month = {mar},
pages = {657–672},
numpages = {16},
keywords = {Adversarial learning, Metric learning, Cross-modal retrieval}
}

@inproceedings{prdh,
author = {Yang, Erkun and Deng, Cheng and Liu, Wei and Liu, Xianglong and Tao, Dacheng and Gao, Xinbo},
title = {Pairwise Relationship Guided Deep Hashing for Cross-Modal Retrieval},
year = {2017},
publisher = {AAAI Press},
abstract = {With benefits of low storage cost and fast query speed, cross-modal hashing has received considerable attention recently. However, almost all existing methods on cross-modal hashing cannot obtain powerful hash codes due to directly utilizing hand-crafted features or ignoring heterogeneous correlations across different modalities, which will greatly degrade the retrieval performance. In this paper, we propose a novel deep cross-modal hashing method to generate compact hash codes through an end-to-end deep learning architecture, which can effectively capture the intrinsic relationships between various modalities. Our architecture integrates different types of pairwise constraints to encourage the similarities of the hash codes from an intra-modal view and an inter-modal view, respectively. Moreover, additional decorrelation constraints are introduced to this architecture, thus enhancing the discriminative ability of each hash bit. Extensive experiments show that our proposed method yields state-of-the-art results on two cross-modal retrieval datasets.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1618–1625},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{cdq, title={Collective Deep Quantization for Efficient Cross-Modal Retrieval}, volume={31}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11218}, abstractNote={ &lt;p&gt; Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across content modalities, e.g., using an image to retrieve for texts. This paper presents a compact coding solution for efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in single-modal similarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to introduce quantization in end-to-end deep architecture for cross-modal retrieval. The major contribution lies in jointly learning deep representations and the quantizers for both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our approach simultaneously learns the common quantizer codebook for both modalities through which the cross-modal correlation can be substantially enhanced. CDQ enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art cross-modal retrieval results on standard benchmarks. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Cao, Yue and Long, Mingsheng and Wang, Jianmin and Liu, Shichen}, year={2017}, month={Feb.} }

@inproceedings{jdsh,
author = {Liu, Song and Qian, Shengsheng and Guan, Yang and Zhan, Jiawei and Ying, Long},
title = {Joint-Modal Distribution-Based Similarity Hashing for Large-Scale Unsupervised Deep Cross-Modal Retrieval},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401086},
doi = {10.1145/3397271.3401086},
abstract = {Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes.To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1379–1388},
numpages = {10},
keywords = {unsupervised cross-modal hashing, cross-modal retrieval, large-scale multimedia retrieval},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{dvsh,
author = {Cao, Yue and Long, Mingsheng and Wang, Jianmin and Yang, Qiang and Yu, Philip S.},
title = {Deep Visual-Semantic Hashing for Cross-Modal Retrieval},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939812},
doi = {10.1145/2939672.2939812},
abstract = {Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and high-quality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1445–1454},
numpages = {10},
keywords = {cross-modal retrieval, multimodal embedding, deep hashing},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@ARTICLE{dch,  author={Xu, Xing and Shen, Fumin and Yang, Yang and Shen, Heng Tao and Li, Xuelong},  journal={IEEE Transactions on Image Processing},   title={Learning Discriminative Binary Codes for Large-scale Cross-modal Retrieval},   year={2017},  volume={26},  number={5},  pages={2494-2507},  doi={10.1109/TIP.2017.2676345}}

@inproceedings{ssah,
  title={Self-supervised adversarial hashing networks for cross-modal retrieval},
  author={Li, Chao and Deng, Cheng and Li, Ning and Liu, Wei and Gao, Xinbo and Tao, Dacheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4242--4251},
  year={2018}
}

@misc{uch,
      title={Coupled CycleGAN: Unsupervised Hashing Network for Cross-Modal Retrieval}, 
      author={Chao Li and Cheng Deng and Lei Wang and De Xie and Xianglong Liu},
      year={2019},
      eprint={1903.02149},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@ARTICLE{lsrh,  author={Li, Kai and Qi, Guo-Jun and Ye, Jun and Hua, Kien A.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Linear Subspace Ranking Hashing for Cross-Modal Retrieval},   year={2017},  volume={39},  number={9},  pages={1825-1838},  doi={10.1109/TPAMI.2016.2610969}}

@INPROCEEDINGS{djsrh,  author={Su, Shupeng and Zhong, Zhisheng and Zhang, Chao},  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},   title={Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval},   year={2019},  volume={},  number={},  pages={3027-3035},  doi={10.1109/ICCV.2019.00312}}




@inproceedings{jgrhml,
author = {Zhai, Xiaohua and Peng, Yuxin and Xiao, Jianguo},
title = {Heterogeneous Metric Learning with Joint Graph Regularization for Cross-Media Retrieval},
year = {2013},
publisher = {AAAI Press},
abstract = {As the major component of big data, unstructured heterogeneous multimedia content such as text, image, audio, video and 3D increasing rapidly on the Internet. User demand a new type of cross-media retrieval where user can search results across various media by submitting query of any media. Since the query and the retrieved results can be of different media, how to learn a heterogeneous metric is the key challenge. Most existing metric learning algorithms only focus on a single media where all of the media objects share the same data representation. In this paper, we propose a joint graph regularized heterogeneous metric learning (JGRHML) algorithm, which integrates the structure of different media into a joint graph regularization. In JGRHML, different media are complementary to each other and optimizing them simultaneously can make the solution smoother for both media and further improve the accuracy of the final metric. Based on the heterogeneous metric, we further learn a high-level semantic metric through label propagation. JGRHML is effective to explore the semantic relationship hidden across different modalities. The experimental results on two datasets with up to five media types show the effectiveness of our proposed approach.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1198–1204},
numpages = {7},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@inproceedings{cca,
  title={Canonical correlation analysis},
  author={Weenink, David},
  booktitle={Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam},
  volume={25},
  pages={81--99},
  year={2003},
  organization={Citeseer}
}

@InProceedings{dcca,
  title = 	 {Deep Canonical Correlation Analysis},
  author = 	 {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1247--1255},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/andrew13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/andrew13.html},
  abstract = 	 {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}

@INPROCEEDINGS{cmfh,  author={Ding, Guiguang and Guo, Yuchen and Zhou, Jile},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},   title={Collective Matrix Factorization Hashing for Multimodal Data},   year={2014},  volume={},  number={},  pages={2083-2090},  doi={10.1109/CVPR.2014.267}}

@CONFERENCE{wiki,
  author = {Rasiwasia, N. and Costa Pereira, J. and Coviello, E. and Doyle, G. and 
	Lanckriet, G.R.G. and Levy, R. and Vasconcelos, N.},
  title = {{A New Approach to Cross-Modal Multimedia Retrieval}},
  booktitle = {ACM International Conference on Multimedia},
  pages={251--260},
  year = {2010}
}

@inproceedings{pascal,
author = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
title = {Collecting Image Annotations Using Amazon's Mechanical Turk},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Crowd-sourcing approaches such as Amazon's Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {139–147},
numpages = {9},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{nuswide,
author = {Chua, Tat-Seng and Tang, Jinhui and Hong, Richang and Li, Haojie and Luo, Zhiping and Zheng, Yantao},
title = {NUS-WIDE: A Real-World Web Image Database from National University of Singapore},
year = {2009},
isbn = {9781605584805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1646396.1646452},
doi = {10.1145/1646396.1646452},
abstract = {This paper introduces a web image dataset created by NUS's Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.},
booktitle = {Proceedings of the ACM International Conference on Image and Video Retrieval},
articleno = {48},
numpages = {9},
keywords = {tag refinement, Flickr, training set construction, annotation, retrieval, web image},
location = {Santorini, Fira, Greece},
series = {CIVR '09}
}

@article{xmedianet,
author = {Peng, Yuxin and Qi, Jinwei and Yuan, Yuxin},
title = {Modality-Specific Cross-Modal Similarity Measurement With Recurrent Attention Network},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {1057-7149},
url = {https://doi.org/10.1109/TIP.2018.2852503},
doi = {10.1109/TIP.2018.2852503},
abstract = {Nowadays, cross-modal retrieval plays an important role to flexibly find useful information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities, such as image and text, have imbalanced and complementary relationship, and they contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on deep neural network mostly construct one common space for different modalities, to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Therefore, we propose modality-specific cross-modal similarity measurement approach by constructing the independent semantic space for each modality, which adopts an end-to-end framework to directly generate the modality-specific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding, which utilizes the learned attention weights for guiding the fine-grained cross-modal correlation learning, and captures the imbalanced and complementary relationship between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modality-specific cross-modal similarities to perform the cross-modal retrieval. Experiments on the widely used Wikipedia, Pascal Sentence, and MS-COCO data sets as well as our constructed large-scale XMediaNet data set verify the effectiveness of our proposed approach, outperforming nine state-of-the-art methods.},
journal = {Trans. Img. Proc.},
month = {nov},
pages = {5585–5599},
numpages = {15}
}

@misc{mscoco,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{clip4cmr,
      title={A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval}, 
      author={Zhixiong Zeng and Wenji Mao},
      year={2022},
      eprint={2201.02772},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{autoencoder,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},

URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647}
,
    abstract = { High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. }
}

@ARTICLE{deep_learning,
  author={Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  journal={Nature}, 
  title={Deep Learning}, 
  year={2015},
  volume={},
  number={},
  pages={436–444},
  doi={https://doi.org/10.1038/nature14539}}

@ARTICLE{algcn,
  author={Qian, Shengsheng and Xue, Dizhan and Fang, Quan and Xu, Changsheng},
  journal={IEEE Transactions on Multimedia}, 
  title={Adaptive Label-aware Graph Convolutional Networks for Cross-Modal Retrieval}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMM.2021.3101642}}
  
@inbook{pan,
author = {Zeng, Zhixiong and Wang, Shuai and Xu, Nan and Mao, Wenji},
title = {PAN: Prototype-Based Adaptive Network for Robust Cross-Modal Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462867},
abstract = {In practical applications of cross-modal retrieval, test queries of the retrieval system may vary greatly and come from unknown category. Meanwhile, due to the cost and difficulty of data collection as well as other issues, the available data for cross-modal retrieval are often imbalanced over different modalities. In this paper, we address two important issues to increase the robustness of cross-modal retrieval system for real-world applications: handling test queries from unknown category and modality-imbalanced training data. The first issue has not been addressed by existing methods and the second issue was not well addressed in the related research. To tackle the above issues, we take the advantage of prototype learning, and propose a prototype-based adaptive network (PAN) for robust cross-modal retrieval. Our method leverages a unified prototype to represent each semantic category across modalities, which provides discriminative information of different categories and takes unified prototypes as anchors to learn cross-modal representations adaptively. Moreover, we propose a novel prototype propagation strategy to reconstruct balanced representations which preserves the semantic consistency and modality heterogeneity. Experimental results on the benchmark datasets demonstrate the effectiveness of our method compared to the SOTA methods, and further robustness tests show the superiority of our method in solving the above issues.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1125–1134},
numpages = {10}
}

@article{mlsph,
title = {Multi-label semantics preserving based deep cross-modal hashing},
journal = {Signal Processing: Image Communication},
volume = {93},
pages = {116131},
year = {2021},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.116131},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520302344},
author = {Xitao Zou and Xinzhi Wang and Erwin M. Bakker and Song Wu},
keywords = {Multi-modal retrieval, Deep cross-modal hashing, Multi-label semantic learning},
abstract = {Due to the storage and retrieval efficiency of hashing, as well as the highly discriminative feature extraction by deep neural networks, deep cross-modal hashing retrieval has been attracting increasing attention in recent years. However, most of existing deep cross-modal hashing methods simply employ single-label to directly measure the semantic relevance across different modalities, but neglect the potential contributions from multiple category labels. With the aim to improve the accuracy of cross-modal hashing retrieval by fully exploring the semantic relevance based on multiple labels of training data, in this paper, we propose a multi-label semantics preserving based deep cross-modal hashing (MLSPH) method. MLSPH firstly utilizes multi-labels of instances to calculate semantic similarity of the original data. Subsequently, a memory bank mechanism is introduced to preserve the multiple labels semantic similarity constraints and enforce the distinctiveness of learned hash representations over the whole training batch. Extensive experiments on several benchmark datasets reveal that the proposed MLSPH surpasses prominent baselines and reaches the state-of-the-art performance in the field of cross-modal hashing retrieval. Code is available at: https://github.com/SWU-CS-MediaLab/MLSPH.}
}

@article{mmach,
title = {Multi-label modality enhanced attention based self-supervised deep cross-modal hashing},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107927},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107927},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101073X},
author = {Xitao Zou and Song Wu and Nian Zhang and Erwin M. Bakker},
keywords = {Deep cross-modal hashing, Attention mechanism, Multi-label semantic learning},
abstract = {The recent deep cross-modal hashing (DCMH) has achieved superior performance in effective and efficient cross-modal retrieval and thus has drawn increasing attention. Nevertheless, there are still two limitations for most existing DCMH methods: (1) single labels are usually leveraged to measure the semantic similarity of cross-modal pairwise instances while neglecting that many cross-modal datasets contain abundant semantic information among multi-labels. (2) several DCMH methods utilized the multi-labels to supervise the learning of hash functions. Nevertheless, the feature space of multi-labels suffers the weakness of sparse, resulting in sub-optimization for the hash functions learning. Thus, this paper proposed a multi-label modality enhanced attention-based self-supervised deep cross-modal hashing (MMACH) framework. Specifically, a multi-label modality enhanced attention module is designed to integrate the significant features from cross-modal data into multi-labels feature representations, aiming to improve its completion. Moreover, a multi-label cross-modal triplet loss is defined based on the criterion that the feature representations of cross-modal pairwise instances with more common categories should preserve higher semantic similarity than other instances. To the best of our knowledge, the multi-label cross-modal triplet loss is the first time designed for cross-modal retrieval. Extensive experiments on four multi-label cross-modal datasets demonstrate the effectiveness and efficiency of our proposed MMACH. Moreover, the MMACH also achieved superior performance and outperformed several state-of-the-art methods on the task of cross-modal retrieval. The source code of MMACH is available at https://github.com/SWU-CS-MediaLab/MMACH.}
}

@misc{clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2016comprehensive,
      title={A Comprehensive Survey on Cross-modal Retrieval}, 
      author={Kaiye Wang and Qiyue Yin and Wei Wang and Shu Wu and Liang Wang},
      year={2016},
      eprint={1607.06215},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}

@inproceedings{avq_2020,
  title={Accelerating Large-Scale Inference with Anisotropic Vector Quantization},
  author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  year={2020},
  URL={https://arxiv.org/abs/1908.10396}
}

@INPROCEEDINGS{GMA,  author={Sharma, Abhishek and Kumar, Abhishek and Daume, Hal and Jacobs, David W.},  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},   title={Generalized Multiview Analysis: A discriminative latent space},   year={2012},  volume={},  number={},  pages={2160-2167},  doi={10.1109/CVPR.2012.6247923}}

@INPROCEEDINGS{ml-CCA,  author={Ranjan, Viresh and Rasiwasia, Nikhil and Jawahar, C. V.},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={Multi-label Cross-Modal Retrieval},   year={2015},  volume={},  number={},  pages={4094-4102},  doi={10.1109/ICCV.2015.466}}

@inproceedings{Peng2016CrossMediaSR,
  title={Cross-Media Shared Representation by Hierarchical Learning with Multiple Deep Networks},
  author={Yuxin Peng and Xin Huang and Jinwei Qi},
  booktitle={IJCAI},
  year={2016}
}

@article{KCCA,
  title={Kernel and Nonlinear Canonical Correlation Analysis},
  author={Pei Ling Lai and Colin Fyfe},
  journal={International journal of neural systems},
  year={2000},
  volume={10 5},
  pages={
          365-77
        }
}

@ARTICLE{JRL,  author={Zhai, Xiaohua and Peng, Yuxin and Xiao, Jianguo},  journal={IEEE Transactions on Circuits and Systems for Video Technology},   title={Learning Cross-Media Joint Representation With Sparse and Semisupervised Regularization},   year={2014},  volume={24},  number={6},  pages={965-978},  doi={10.1109/TCSVT.2013.2276704}}




@article{ccl,
  title={CCL: Cross-modal Correlation Learning With Multigrained Fusion by Hierarchical Network},
  author={Yuxin Peng and Jinwei Qi and Xin Huang and Yuxin Yuan},
  journal={IEEE Transactions on Multimedia},
  year={2018},
  volume={20},
  pages={405-420}
}


@article{Cao2021GlobalRA,
  title={Global Relation-Aware Attention Network for Image-Text Retrieval},
  author={Jie Cao and Shengsheng Qian and Huaiwen Zhang and Quan Fang and Changsheng Xu},
  journal={Proceedings of the 2021 International Conference on Multimedia Retrieval},
  year={2021}
}

@article{Qu2021DynamicMI,
  title={Dynamic Modality Interaction Modeling for Image-Text Retrieval},
  author={Leigang Qu and Meng Liu and Jianlong Wu and Zan Gao and Liqiang Nie},
  journal={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2021}
}

@article{Chen2020IMRAMIM,
  title={IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval},
  author={Hui Chen and Guiguang Ding and Xudong Liu and Zijia Lin and Ji Liu and Jungong Han},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={12652-12660}
}

@InProceedings{Lee2018StackedCA,
author = {Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
title = {Stacked Cross Attention for Image-Text Matching},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}


@article{Wang2016JointFS,
  title={Joint Feature Selection and Subspace Learning for Cross-Modal Retrieval},
  author={K. Wang and Ran He and Liang Wang and Wei Wang and Tieniu Tan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2016},
  volume={38},
  pages={2010-2023}
}

@article{Wu2017JointLS,
  title={Joint Latent Subspace Learning and Regression for Cross-Modal Retrieval},
  author={Jianlong Wu and Zhouchen Lin and Hongbin Zha},
  journal={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2017}
}

@article{Peng2018ModalitySpecificCS,
  title={Modality-Specific Cross-Modal Similarity Measurement With Recurrent Attention Network},
  author={Yuxin Peng and Jinwei Qi and Yuxin Yuan},
  journal={IEEE Transactions on Image Processing},
  year={2018},
  volume={27},
  pages={5585-5599}
}

@article{Peng2021CMGANsCG,
author = {Peng, Yuxin and Qi, Jinwei},
title = {CM-GANs: Cross-Modal Generative Adversarial Networks for Common Representation Learning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3284750},
doi = {10.1145/3284750},
abstract = {It is known that the inconsistent distributions and representations of different modalities, such as image and text, cause the heterogeneity gap, which makes it very challenging to correlate heterogeneous data and measure their similarities. Recently, generative adversarial networks (GANs) have been proposed and have shown their strong ability to model data distribution and learn discriminative representation. It has also been shown that adversarial learning can be fully exploited to learn discriminative common representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively correlate large-scale heterogeneous data of different modalities with the power of GANs to model cross-modal joint distribution. In this article, we propose Cross-modal Generative Adversarial Networks (CM-GANs) with the following contributions. First, a cross-modal GAN architecture is proposed to model joint distribution over the data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both compete with each other to promote cross-modal correlation learning. Second, the cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form the generative model. They not only exploit the cross-modal correlation for learning the common representations but also preserve reconstruction information for capturing the semantic consistency within each modality. Third, a cross-modal adversarial training mechanism is proposed, which uses two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make the generated common representations more discriminative by the adversarial training process. In summary, our proposed CM-GAN approach can use GANs to perform cross-modal common representation learning by which the heterogeneous data can be effectively correlated. Extensive experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval compared with 13 state-of-the-art methods on 4 cross-modal datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {22},
numpages = {24},
keywords = {cross-modal retrieval, common representation learning, cross-modal adversarial mechanism, Generative adversarial network}
}



@inproceedings{Li2003MultimediaCP,
  title={Multimedia content processing through cross-modal association},
  author={Dongge Li and Nevenka Dimitrova and Mingkun Li and Ishwar K. Sethi},
  booktitle={MULTIMEDIA '03},
  year={2003}
}

@article{Ding2005ClassificationUG,
  title={Classification Using Generalized Partial Least Squares},
  author={B. Ding and Robert Gentleman},
  journal={Journal of Computational and Graphical Statistics},
  year={2005},
  volume={14},
  pages={280 - 298}
}

@inproceedings{Srivastava2012LearningRF,
  title={Learning representations for multimodal data with deep belief nets},
  author={Srivastava, Nitish and Salakhutdinov, Ruslan},
  booktitle={International conference on machine learning workshop},
  volume={79},
  pages={3},
  year={2012}
}

@article{Wang2021DeepCD,
  title={Deep Collaborative Discrete Hashing With Semantic-Invariant Structure Construction},
  author={Zijian Wang and Zheng Zhang and Yadan Luo and Zi Huang and Heng Tao Shen},
  journal={IEEE Transactions on Multimedia},
  year={2021},
  volume={23},
  pages={1274-1286}
}

@inproceedings{Xu2019GraphCN,
  title={Graph Convolutional Network Hashing for Cross-Modal Retrieval},
  author={Ruiqing Xu and Chao Li and Junchi Yan and Cheng Deng and Xianglong Liu},
  booktitle={IJCAI},
  year={2019}
}

@article{tsne,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605}
}

@article{Peng2018AnOO,
author = {Peng, Yuxin and Huang, Xin and Zhao, Yunzhen},
title = {An Overview of Cross-Media Retrieval: Concepts, Methodologies, Benchmarks, and Challenges},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {28},
number = {9},
issn = {1051-8215},
url = {https://doi.org/10.1109/TCSVT.2017.2705068},
doi = {10.1109/TCSVT.2017.2705068},
abstract = {Multimedia retrieval plays an indispensable role in big data utilization. Past efforts mainly focused on single-media retrieval. However, the requirements of users are highly flexible, such as retrieving the relevant audio clips with one query of image. So challenges stemming from the “media gap,” which means that representations of different media types are inconsistent, have attracted increasing attention. Cross-media retrieval is designed for the scenarios where the queries and retrieval results are of different media types. As a relatively new research topic, its concepts, methodologies, and benchmarks are still not clear in the literature. To address these issues, we review more than 100 references, give an overview including the concepts, methodologies, major challenges, and open issues, as well as build up the benchmarks, including data sets and experimental results. Researchers can directly adopt the benchmarks to promptly evaluate their proposed methods. This will help them to focus on algorithm design, rather than the time-consuming compared methods and results. It is noted that we have constructed a new data set XMedia, which is the first publicly available data set with up to five media types (text, image, video, audio, and 3-D model). We believe this overview will attract more researchers to focus on cross-media retrieval and be helpful to them.},
journal = {IEEE Trans. Cir. and Sys. for Video Technol.},
month = {sep},
pages = {2372–2385},
numpages = {14}
}


@inproceedings{dwijesh,
author = {Gohil, Dwijesh and Bithel, Shivangi and Bedathur, Srikanta},
title = {LCM: A Surprisingly Effective Framework for Supervised Cross-Modal Retrieval},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571048},
doi = {10.1145/3570991.3571048},
abstract = {Due to its increasing importance, cross-modal retrieval (CMR), where the query from one modality is used to retrieve objects from a different modality, has gained a lot of attention. A plethora of techniques have been proposed for this task, with deep learnt multi-modal models being the dominant paradigm. While these techniques have become increasingly sophisticated in terms of learning representations of multi-modal objects in a common space, relatively less attention is paid to the overall computational costs involved while training the model and during retrieval. In this work, we present LCM (Lightweight framework for Cross-Modal retrieval), a surprisingly effective approach with very low computational costs. It can work with any uni- and multi-modal representations that is available ranging from BoW/GIST to CLIP for text/image modality. In its training phase, LCM exploits the semantic labels with a combination of shallow modality-specific feed-forward network and a label auto-encoder such that embeddings in the common representation space that share labels are close to each other. During retrieval, LCM employs a novel 2-stage nearest neighbor (2Sknn) search to first rank candidate labels that are relevant to a query (stage-1), and then use this ranking to retrieve results from the indexed collection (stage-2). Experiments over 6 popular uni- and multi-label supervised CMR benchmarks show that LCM outperforms some of the very recent strong baselines by upto 20\% gains in mAP values. Furthermore, we show that 2Sknn can benefit other baseline methods as well offering upto 50\% mAP gains in some cases.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science \& Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {37–46},
numpages = {10},
keywords = {Cross-modal retrieval, Representation Learning, 2-Stage Retrieval},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@inproceedings{sbithel,
author = {Bithel, Shivangi and Bedathur, Srikanta},
title = {Evaluating Cross-Modal Generative Models Using Retrieval Task},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591979},
doi = {10.1145/3539618.3591979},
abstract = {Generative models have taken the world by storm -- image generative models such as Stable Diffusion and DALL-E generate photo-realistic images, whereas image captioning models such as BLIP, GIT, ClipCap, and ViT-GPT2 generate descriptive and informative captions. While it may be true that these models produce remarkable results, their systematic evaluation is missing, making it hard to advance the research further. Currently, heuristic metrics such as the Inception Score and the Fr\'{e}chet Inception Distance are the most prevalent metrics for the image generation task, while BLEU, CIDEr, SPICE, METEOR, BERTScore, and CLIPScore are common for the image captioning task. Unfortunately, these are poorly interpretable and are not based on the solid user-behavior model that the Information Retrieval community has worked towards. In this paper, we present a novel cross-modal retrieval framework to evaluate the effectiveness of cross-modal (image-to-text and text-to-image) generative models using reference text and images. We propose the use of scoring models based on user-behavior, such as Normalized Discounted Cumulative Gain (nDCG'@K ) and Rank-Biased Precision (RBP'@K) adjusted for incomplete judgments. Experiments using ECCV Caption and Flickr8k-EXPERTS benchmark datasets demonstrate the effectiveness of various image captioning and image generation models for the proposed retrieval task. Results also indicate that the nDCG'@K and RBP'@K scores are consistent with heuristics-driven metrics, excluding CLIPScore, in model selection.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1960–1965},
numpages = {6},
keywords = {cross-modal retrieval, cross-modal generative modal, evaluation method},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{VILT,
  author       = {Wonjae Kim and
                  Bokyung Son and
                  Ildoo Kim},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {ViLT: Vision-and-Language Transformer Without Convolution or Region
                  Supervision},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {5583--5594},
  publisher    = {{PMLR}},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v139/kim21k.html},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/KimSK21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{BLIP,
  author       = {Junnan Li and
                  Dongxu Li and
                  Caiming Xiong and
                  Steven C. H. Hoi},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language
                  Understanding and Generation},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {12888--12900},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/li22n.html},
  timestamp    = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0001LXH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bleu,
  author       = {Kishore Papineni and
                  Salim Roukos and
                  Todd Ward and
                  Wei{-}Jing Zhu},
  title        = {Bleu: a Method for Automatic Evaluation of Machine Translation},
  booktitle    = {Proceedings of the 40th Annual Meeting of the Association for Computational
                  Linguistics, July 6-12, 2002, Philadelphia, PA, {USA}},
  pages        = {311--318},
  publisher    = {{ACL}},
  year         = {2002},
  url          = {https://aclanthology.org/P02-1040/},
  doi          = {10.3115/1073083.1073135},
  timestamp    = {Fri, 06 Aug 2021 00:40:58 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/PapineniRWZ02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{GIT,
  doi = {10.48550/ARXIV.2205.14100},
  url = {https://arxiv.org/abs/2205.14100},
  author = {Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {GIT: A Generative Image-to-text Transformer for Vision and Language},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{ClipCap,
  author       = {Ron Mokady and
                  Amir Hertz and
                  Amit H. Bermano},
  title        = {ClipCap: {CLIP} Prefix for Image Captioning},
  journal      = {CoRR},
  volume       = {abs/2111.09734},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.09734},
  eprinttype    = {arXiv},
  eprint       = {2111.09734},
  timestamp    = {Mon, 22 Nov 2021 16:44:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-09734.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{MAPL,
  author       = {Oscar Ma{\~{n}}as and
                  Pau Rodr{\'{\i}}guez and
                  Saba Ahmadi and
                  Aida Nematzadeh and
                  Yash Goyal and
                  Aishwarya Agrawal},
  title        = {{MAPL:} Parameter-Efficient Adaptation of Unimodal Pre-Trained Models
                  for Vision-Language Few-Shot Prompting},
  journal      = {CoRR},
  volume       = {abs/2210.07179},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2210.07179},
  doi          = {10.48550/arXiv.2210.07179},
  eprinttype    = {arXiv},
  eprint       = {2210.07179},
  timestamp    = {Tue, 18 Oct 2022 15:06:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-07179.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{VIT,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{GPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}
@inproceedings{CIDEr,
  author       = {Ramakrishna Vedantam and
                  C. Lawrence Zitnick and
                  Devi Parikh},
  title        = {CIDEr: Consensus-based image description evaluation},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2015, Boston, MA, USA, June 7-12, 2015},
  pages        = {4566--4575},
  publisher    = {{IEEE} Computer Society},
  year         = {2015},
  url          = {https://doi.org/10.1109/CVPR.2015.7299087},
  doi          = {10.1109/CVPR.2015.7299087},
  timestamp    = {Fri, 24 Mar 2023 00:02:53 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/VedantamZP15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{METEOR,
  author       = {Satanjeev Banerjee and
                  Alon Lavie},
  editor       = {Jade Goldstein and
                  Alon Lavie and
                  Chin{-}Yew Lin and
                  Clare R. Voss},
  title        = {{METEOR:} An Automatic Metric for {MT} Evaluation with Improved Correlation
                  with Human Judgments},
  booktitle    = {Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation
                  Measures for Machine Translation and/or Summarization@ACL 2005, Ann
                  Arbor, Michigan, USA, June 29, 2005},
  pages        = {65--72},
  publisher    = {Association for Computational Linguistics},
  year         = {2005},
  url          = {https://aclanthology.org/W05-0909/},
  timestamp    = {Fri, 06 Aug 2021 00:40:57 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/BanerjeeL05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ROUGE,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Chin-Yew Lin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2004}
}
@inproceedings{BERTScore,
  author       = {Tianyi Zhang and
                  Varsha Kishore and
                  Felix Wu and
                  Kilian Q. Weinberger and
                  Yoav Artzi},
  title        = {BERTScore: Evaluating Text Generation with {BERT}},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SkeHuCVFDr},
  timestamp    = {Wed, 03 Jun 2020 10:08:32 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{CLIPScore,
  author       = {Jack Hessel and
                  Ari Holtzman and
                  Maxwell Forbes and
                  Ronan Le Bras and
                  Yejin Choi},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {CLIPScore: {A} Reference-free Evaluation Metric for Image Captioning},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
                  Republic, 7-11 November, 2021},
  pages        = {7514--7528},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.emnlp-main.595},
  doi          = {10.18653/v1/2021.emnlp-main.595},
  timestamp    = {Mon, 05 Dec 2022 16:59:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/HesselHFBC21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ECCV,
  author       = {Sanghyuk Chun and
                  Wonjae Kim and
                  Song Park and
                  Minsuk Chang and
                  Seong Joon Oh},
  editor       = {Shai Avidan and
                  Gabriel J. Brostow and
                  Moustapha Ciss{\'{e}} and
                  Giovanni Maria Farinella and
                  Tal Hassner},
  title        = {{ECCV} Caption: Correcting False Negatives by Collecting Machine-and-Human-verified
                  Image-Caption Associations for {MS-COCO}},
  booktitle    = {Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv,
                  Israel, October 23-27, 2022, Proceedings, Part {VIII}},
  series       = {Lecture Notes in Computer Science},
  volume       = {13668},
  pages        = {1--19},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-3-031-20074-8\_1},
  doi          = {10.1007/978-3-031-20074-8\_1},
  timestamp    = {Wed, 16 Nov 2022 21:55:17 +0100},
  biburl       = {https://dblp.org/rec/conf/eccv/ChunKPCO22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{flickr8k,
  author       = {Micah Hodosh and
                  Peter Young and
                  Julia Hockenmaier},
  title        = {Framing Image Description as a Ranking Task: Data, Models and Evaluation
                  Metrics},
  journal      = {J. Artif. Intell. Res.},
  volume       = {47},
  pages        = {853--899},
  year         = {2013},
  url          = {https://doi.org/10.1613/jair.3994},
  doi          = {10.1613/jair.3994},
  timestamp    = {Mon, 21 Jan 2019 15:01:17 +0100},
  biburl       = {https://dblp.org/rec/journals/jair/HodoshYH13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{stable-diffusion,
  author       = {Robin Rombach and
                  Andreas Blattmann and
                  Dominik Lorenz and
                  Patrick Esser and
                  Bj{\"{o}}rn Ommer},
  title        = {High-Resolution Image Synthesis with Latent Diffusion Models},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages        = {10674--10685},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/CVPR52688.2022.01042},
  doi          = {10.1109/CVPR52688.2022.01042},
  timestamp    = {Wed, 05 Oct 2022 16:31:19 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/RombachBLEO22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{minDALL-E,
  title         = {minDALL-E on Conceptual Captions},
  author        = {Saehoon Kim and Sanghun Cho and Chiheon Kim and Doyup Lee and Woonhyuk Baek},
  year          = {2021},
  howpublished  = {\url{https://github.com/kakaobrain/minDALL-E}}
}
@inproceedings{cc3m,
  author       = {Piyush Sharma and
                  Nan Ding and
                  Sebastian Goodman and
                  Radu Soricut},
  editor       = {Iryna Gurevych and
                  Yusuke Miyao},
  title        = {Conceptual Captions: {A} Cleaned, Hypernymed, Image Alt-text Dataset
                  For Automatic Image Captioning},
  booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
                  1: Long Papers},
  pages        = {2556--2565},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://aclanthology.org/P18-1238/},
  doi          = {10.18653/v1/P18-1238},
  timestamp    = {Tue, 16 Aug 2022 23:04:35 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/SoricutDSG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{cc12m,
  author       = {Soravit Changpinyo and
                  Piyush Sharma and
                  Nan Ding and
                  Radu Soricut},
  title        = {Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize
                  Long-Tail Visual Concepts},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2021, virtual, June 19-25, 2021},
  pages        = {3558--3568},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2021},
  url          = {https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo\_Conceptual\_12M\_Pushing\_Web-Scale\_Image-Text\_Pre-Training\_To\_Recognize\_Long-Tail\_Visual\_CVPR\_2021\_paper.html},
  doi          = {10.1109/CVPR46437.2021.00356},
  timestamp    = {Mon, 18 Jul 2022 16:47:40 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/ChangpinyoSDS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{LAION5B,
  author       = {Christoph Schuhmann and
                  Romain Beaumont and
                  Richard Vencu and
                  Cade Gordon and
                  Ross Wightman and
                  Mehdi Cherti and
                  Theo Coombes and
                  Aarush Katta and
                  Clayton Mullis and
                  Mitchell Wortsman and
                  Patrick Schramowski and
                  Srivatsa Kundurthy and
                  Katherine Crowson and
                  Ludwig Schmidt and
                  Robert Kaczmarczyk and
                  Jenia Jitsev},
  title        = {{LAION-5B:} An open large-scale dataset for training next generation
                  image-text models},
  journal      = {CoRR},
  volume       = {abs/2210.08402},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2210.08402},
  doi          = {10.48550/arXiv.2210.08402},
  eprinttype    = {arXiv},
  eprint       = {2210.08402},
  timestamp    = {Wed, 19 Oct 2022 12:47:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-08402.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Betzalel2022ASO,
  author       = {Eyal Betzalel and
                  Coby Penso and
                  Aviv Navon and
                  Ethan Fetaya},
  title        = {A Study on the Evaluation of Generative Models},
  journal      = {CoRR},
  volume       = {abs/2206.10935},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2206.10935},
  doi          = {10.48550/arXiv.2206.10935},
  eprinttype    = {arXiv},
  eprint       = {2206.10935},
  timestamp    = {Mon, 27 Jun 2022 16:51:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2206-10935.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ImageNet,
  author       = {Alex Krizhevsky and
                  Ilya Sutskever and
                  Geoffrey E. Hinton},
  editor       = {Peter L. Bartlett and
                  Fernando C. N. Pereira and
                  Christopher J. C. Burges and
                  L{\'{e}}on Bottou and
                  Kilian Q. Weinberger},
  title        = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle    = {Advances in Neural Information Processing Systems 25: 26th Annual
                  Conference on Neural Information Processing Systems 2012. Proceedings
                  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
  pages        = {1106--1114},
  year         = {2012},
  url          = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/KrizhevskySH12.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DALLE,
  author       = {Aditya Ramesh and
                  Mikhail Pavlov and
                  Gabriel Goh and
                  Scott Gray and
                  Chelsea Voss and
                  Alec Radford and
                  Mark Chen and
                  Ilya Sutskever},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Zero-Shot Text-to-Image Generation},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {8821--8831},
  publisher    = {{PMLR}},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v139/ramesh21a.html},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/RameshPGGVRCS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DALLE2,
  author       = {Aditya Ramesh and
                  Prafulla Dhariwal and
                  Alex Nichol and
                  Casey Chu and
                  Mark Chen},
  title        = {Hierarchical Text-Conditional Image Generation with {CLIP} Latents},
  journal      = {CoRR},
  volume       = {abs/2204.06125},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2204.06125},
  doi          = {10.48550/arXiv.2204.06125},
  eprinttype    = {arXiv},
  eprint       = {2204.06125},
  timestamp    = {Tue, 19 Apr 2022 17:11:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2204-06125.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{GLIDE,
  author       = {Alexander Quinn Nichol and
                  Prafulla Dhariwal and
                  Aditya Ramesh and
                  Pranav Shyam and
                  Pamela Mishkin and
                  Bob McGrew and
                  Ilya Sutskever and
                  Mark Chen},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {{GLIDE:} Towards Photorealistic Image Generation and Editing with
                  Text-Guided Diffusion Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {16784--16804},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/nichol22a.html},
  timestamp    = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/NicholDRSMMSC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{MakeAScene,
  author       = {Oran Gafni and
                  Adam Polyak and
                  Oron Ashual and
                  Shelly Sheynin and
                  Devi Parikh and
                  Yaniv Taigman},
  editor       = {Shai Avidan and
                  Gabriel J. Brostow and
                  Moustapha Ciss{\'{e}} and
                  Giovanni Maria Farinella and
                  Tal Hassner},
  title        = {Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors},
  booktitle    = {Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv,
                  Israel, October 23-27, 2022, Proceedings, Part {XV}},
  series       = {Lecture Notes in Computer Science},
  volume       = {13675},
  pages        = {89--106},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-3-031-19784-0\_6},
  doi          = {10.1007/978-3-031-19784-0\_6},
  timestamp    = {Thu, 03 Nov 2022 12:18:06 +0100},
  biburl       = {https://dblp.org/rec/conf/eccv/GafniPASPT22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{IMAGEN,
  author       = {Chitwan Saharia and
                  William Chan and
                  Saurabh Saxena and
                  Lala Li and
                  Jay Whang and
                  Emily Denton and
                  Seyed Kamyar Seyed Ghasemipour and
                  Burcu Karagol Ayan and
                  S. Sara Mahdavi and
                  Rapha Gontijo Lopes and
                  Tim Salimans and
                  Jonathan Ho and
                  David J. Fleet and
                  Mohammad Norouzi},
  title        = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  journal      = {CoRR},
  volume       = {abs/2205.11487},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2205.11487},
  doi          = {10.48550/arXiv.2205.11487},
  eprinttype    = {arXiv},
  eprint       = {2205.11487},
  timestamp    = {Mon, 30 May 2022 15:47:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2205-11487.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Lin2014MicrosoftCC,
  author       = {Tsung{-}Yi Lin and
                  Michael Maire and
                  Serge J. Belongie and
                  James Hays and
                  Pietro Perona and
                  Deva Ramanan and
                  Piotr Doll{\'{a}}r and
                  C. Lawrence Zitnick},
  editor       = {David J. Fleet and
                  Tom{\'{a}}s Pajdla and
                  Bernt Schiele and
                  Tinne Tuytelaars},
  title        = {Microsoft {COCO:} Common Objects in Context},
  booktitle    = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
                  Switzerland, September 6-12, 2014, Proceedings, Part {V}},
  series       = {Lecture Notes in Computer Science},
  volume       = {8693},
  pages        = {740--755},
  publisher    = {Springer},
  year         = {2014},
  url          = {https://doi.org/10.1007/978-3-319-10602-1\_48},
  doi          = {10.1007/978-3-319-10602-1\_48},
  timestamp    = {Thu, 23 Jun 2022 19:55:44 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/LinMBHPRDZ14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Yuan2021FlorenceAN,
  author       = {Lu Yuan and
                  Dongdong Chen and
                  Yi{-}Ling Chen and
                  Noel Codella and
                  Xiyang Dai and
                  Jianfeng Gao and
                  Houdong Hu and
                  Xuedong Huang and
                  Boxin Li and
                  Chunyuan Li and
                  Ce Liu and
                  Mengchen Liu and
                  Zicheng Liu and
                  Yumao Lu and
                  Yu Shi and
                  Lijuan Wang and
                  Jianfeng Wang and
                  Bin Xiao and
                  Zhen Xiao and
                  Jianwei Yang and
                  Michael Zeng and
                  Luowei Zhou and
                  Pengchuan Zhang},
  title        = {Florence: {A} New Foundation Model for Computer Vision},
  journal      = {CoRR},
  volume       = {abs/2111.11432},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.11432},
  eprinttype    = {arXiv},
  eprint       = {2111.11432},
  timestamp    = {Wed, 28 Dec 2022 08:06:20 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-11432.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Anderson2016SPICESP,
  author       = {Peter Anderson and
                  Basura Fernando and
                  Mark Johnson and
                  Stephen Gould},
  editor       = {Bastian Leibe and
                  Jiri Matas and
                  Nicu Sebe and
                  Max Welling},
  title        = {{SPICE:} Semantic Propositional Image Caption Evaluation},
  booktitle    = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
                  The Netherlands, October 11-14, 2016, Proceedings, Part {V}},
  series       = {Lecture Notes in Computer Science},
  volume       = {9909},
  pages        = {382--398},
  publisher    = {Springer},
  year         = {2016},
  url          = {https://doi.org/10.1007/978-3-319-46454-1\_24},
  doi          = {10.1007/978-3-319-46454-1\_24},
  timestamp    = {Wed, 07 Dec 2022 23:10:23 +0100},
  biburl       = {https://dblp.org/rec/conf/eccv/AndersonFJG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Goodfellow2014GenerativeAN,
  author       = {Ian J. Goodfellow and
                  Jean Pouget{-}Abadie and
                  Mehdi Mirza and
                  Bing Xu and
                  David Warde{-}Farley and
                  Sherjil Ozair and
                  Aaron C. Courville and
                  Yoshua Bengio},
  editor       = {Zoubin Ghahramani and
                  Max Welling and
                  Corinna Cortes and
                  Neil D. Lawrence and
                  Kilian Q. Weinberger},
  title        = {Generative Adversarial Nets},
  booktitle    = {Advances in Neural Information Processing Systems 27: Annual Conference
                  on Neural Information Processing Systems 2014, December 8-13 2014,
                  Montreal, Quebec, Canada},
  pages        = {2672--2680},
  year         = {2014},
  url          = {https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/GoodfellowPMXWOCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Kingma2013AutoEncodingVB,
  author       = {Diederik P. Kingma and
                  Max Welling},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Auto-Encoding Variational Bayes},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6114},
  timestamp    = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{JimenezRezende2015VariationalIW,
  author       = {Danilo Jimenez Rezende and
                  Shakir Mohamed},
  editor       = {Francis R. Bach and
                  David M. Blei},
  title        = {Variational Inference with Normalizing Flows},
  booktitle    = {Proceedings of the 32nd International Conference on Machine Learning,
                  {ICML} 2015, Lille, France, 6-11 July 2015},
  series       = {{JMLR} Workshop and Conference Proceedings},
  volume       = {37},
  pages        = {1530--1538},
  publisher    = {JMLR.org},
  year         = {2015},
  url          = {http://proceedings.mlr.press/v37/rezende15.html},
  timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/RezendeM15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Musgrave2020AML,
  author       = {Kevin Musgrave and
                  Serge J. Belongie and
                  Ser{-}Nam Lim},
  editor       = {Andrea Vedaldi and
                  Horst Bischof and
                  Thomas Brox and
                  Jan{-}Michael Frahm},
  title        = {A Metric Learning Reality Check},
  booktitle    = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
                  UK, August 23-28, 2020, Proceedings, Part {XXV}},
  series       = {Lecture Notes in Computer Science},
  volume       = {12370},
  pages        = {681--699},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-58595-2\_41},
  doi          = {10.1007/978-3-030-58595-2\_41},
  timestamp    = {Thu, 23 Jun 2022 19:55:45 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/MusgraveBL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Moffat2008RankbiasedPF,
  author       = {Alistair Moffat and
                  Justin Zobel},
  title        = {Rank-biased precision for measurement of retrieval effectiveness},
  journal      = {{ACM} Trans. Inf. Syst.},
  volume       = {27},
  number       = {1},
  pages        = {2:1--2:27},
  year         = {2008},
  url          = {https://doi.org/10.1145/1416950.1416952},
  doi          = {10.1145/1416950.1416952},
  timestamp    = {Tue, 06 Nov 2018 12:51:56 +0100},
  biburl       = {https://dblp.org/rec/journals/tois/MoffatZ08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Sakai2007AlternativesTB,
  author       = {Tetsuya Sakai},
  editor       = {Wessel Kraaij and
                  Arjen P. de Vries and
                  Charles L. A. Clarke and
                  Norbert Fuhr and
                  Noriko Kando},
  title        = {Alternatives to Bpref},
  booktitle    = {{SIGIR} 2007: Proceedings of the 30th Annual International {ACM} {SIGIR}
                  Conference on Research and Development in Information Retrieval, Amsterdam,
                  The Netherlands, July 23-27, 2007},
  pages        = {71--78},
  publisher    = {{ACM}},
  year         = {2007},
  url          = {https://doi.org/10.1145/1277741.1277756},
  doi          = {10.1145/1277741.1277756},
  timestamp    = {Tue, 06 Nov 2018 11:07:23 +0100},
  biburl       = {https://dblp.org/rec/conf/sigir/Sakai07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Li2022DoDA,
  author       = {Hang Li and
                  Jindong Gu and
                  Rajat Koner and
                  Sahand Sharifzadeh and
                  Volker Tresp},
  title        = {Do {DALL-E} and Flamingo Understand Each Other?},
  journal      = {CoRR},
  volume       = {abs/2212.12249},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.12249},
  doi          = {10.48550/arXiv.2212.12249},
  eprinttype    = {arXiv},
  eprint       = {2212.12249},
  timestamp    = {Wed, 25 Jan 2023 17:29:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-12249.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Kim2022MutualID,
  author       = {Jin{-}Hwa Kim and
                  Yunji Kim and
                  Jiyoung Lee and
                  Kang Min Yoo and
                  Sang{-}Woo Lee},
  title        = {Mutual Information Divergence: {A} Unified Metric for Multimodal Generative
                  Models},
  journal      = {CoRR},
  volume       = {abs/2205.13445},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2205.13445},
  doi          = {10.48550/arXiv.2205.13445},
  eprinttype    = {arXiv},
  eprint       = {2205.13445},
  timestamp    = {Tue, 31 May 2022 15:14:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2205-13445.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Sakai2008OnIR,
  author       = {Tetsuya Sakai and
                  Noriko Kando},
  title        = {On information retrieval metrics designed for evaluation with incomplete
                  relevance assessments},
  journal      = {Inf. Retr.},
  volume       = {11},
  number       = {5},
  pages        = {447--470},
  year         = {2008},
  url          = {https://doi.org/10.1007/s10791-008-9059-7},
  doi          = {10.1007/s10791-008-9059-7},
  timestamp    = {Thu, 14 Oct 2021 09:13:06 +0200},
  biburl       = {https://dblp.org/rec/journals/ir/SakaiK08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{seed,
  author       = {David Picard},
  title        = {Torch.manual{\_}seed(3407) is all you need: On the influence of random
                  seeds in deep learning architectures for computer vision},
  journal      = {CoRR},
  volume       = {abs/2109.08203},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.08203},
  eprinttype    = {arXiv},
  eprint       = {2109.08203},
  timestamp    = {Wed, 22 Sep 2021 14:16:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-08203.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{ndcg,
author = {J\"{a}rvelin, Kalervo and Kek\"{a}l\"{a}inen, Jaana},
title = {Cumulated Gain-Based Evaluation of IR Techniques},
year = {2002},
issue_date = {October 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/582415.582418},
doi = {10.1145/582415.582418},
abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
pages = {422–446},
numpages = {25},
keywords = {cumulated gain, Graded relevance judgments}
}
@article{sakai2009,
author = {Sakai, Tetsuya},
title = {On Fuhr's Guideline for IR Evaluation},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3451964.3451976},
doi = {10.1145/3451964.3451976},
abstract = {In the December 2017 issue of SIGIR Forum, Fuhr presented ten "Thou Shalt Not"s (i.e., warnings against bad practices) for IR experimenters. While his article provides a lot of good materials for discussion, the objective of the present article is to argue that not all of his recommendations should be considered as absolute truths: researchers should be aware that there are other views; conference programme chairs and journal editors should be very careful when providing a guideline for evaluation practices.},
journal = {SIGIR Forum},
month = {feb},
articleno = {12},
numpages = {8}
}
@misc{CLIP_prefix_caption,
  title         = {CLIPCAP Model},
  year          = {2022},
  howpublished  = {\url{https://github.com/rmokady/CLIP_prefix_caption}}
}
@misc{GIT_model,
  title         = {GIT Model},
  year          = {2022},
  howpublished  = {\url{https://huggingface.co/microsoft/git-base-coco}}
}
@misc{ViT_GPT2_Model,
  title         = {ViT-GPT2 Model},
  year          = {2022},
  howpublished  = {\url{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}}
}
@misc{blip_model,
  title         = {BLIP Model},
  year          = {2022},
  howpublished  = {\url{https://github.com/salesforce/BLIP}}
}
@misc{sd,
  title         = {Stable Diffusion Model},
  year          = {2022},
  howpublished  = {\url{https://huggingface.co/stabilityai/stable-diffusion-2-1}}
}
@inproceedings{Frozen,
  author       = {Maria Tsimpoukelli and
                  Jacob Menick and
                  Serkan Cabi and
                  S. M. Ali Eslami and
                  Oriol Vinyals and
                  Felix Hill},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Multimodal Few-Shot Learning with Frozen Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {200--212},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:46 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/TsimpoukelliMCE21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{barratt2018note,
      title={A Note on the Inception Score}, 
      author={Shane Barratt and Rishi Sharma},
      year={2018},
      eprint={1801.01973},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@inproceedings{FID,
  author       = {Martin Heusel and
                  Hubert Ramsauer and
                  Thomas Unterthiner and
                  Bernhard Nessler and
                  Sepp Hochreiter},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
                  Equilibrium},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {6626--6637},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/HeuselRUNH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{CHEN2021195,
title = {New Ideas and Trends in Deep Multimodal Content Understanding: A Review},
journal = {Neurocomputing},
volume = {426},
pages = {195-215},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.10.042},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220315939},
author = {Wei Chen and Weiping Wang and Li Liu and Michael S. Lew},
keywords = {Multimodal deep learning, Ideas and trends, Content understanding, Literature review},
abstract = {The focus of this survey is on the analysis of two modalities of multimodal deep learning: image and text. Unlike classic reviews of deep learning where monomodal image classifiers such as VGG, ResNet and Inception module are central topics, this paper will examine recent multimodal deep models and structures, including auto-encoders, generative adversarial nets and their variants. These models go beyond the simple image classifiers in which they can do uni-directional (e.g. image captioning, image generation) and bi-directional (e.g. cross-modal retrieval, visual question answering) multimodal tasks. Besides, we analyze two aspects of the challenge in terms of better content understanding in deep multimodal applications. We then introduce current ideas and trends in deep multimodal feature learning, such as feature embedding approaches and objective function design, which are crucial in overcoming the aforementioned challenges. Finally, we include several promising directions for future research.}
}
@InProceedings{Karpathy_2014_CVPR,
author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
title = {Large-scale Video Classification with Convolutional Neural Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}
@ARTICLE{joint,
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Lin, Jie and Zhen, Liangli and Peng, Dezhong},
  journal={IEEE Transactions on Cybernetics}, 
  title={Joint Versus Independent Multiview Hashing for Cross-View Retrieval}, 
  year={2021},
  volume={51},
  number={10},
  pages={4982-4993},
  doi={10.1109/TCYB.2020.3027614}}

@inproceedings{tapas,
    title = "{T}a{P}as: Weakly Supervised Table Parsing via Pre-training",
    author = {Herzig, Jonathan  and
      Nowak, Pawel Krzysztof  and
      M{\"u}ller, Thomas  and
      Piccinno, Francesco  and
      Eisenschlos, Julian},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.398",
    doi = "10.18653/v1/2020.acl-main.398",
    pages = "4320--4333",
    abstract = "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT{'}s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
}

@inproceedings{cao,
  title     = {Image-text Retrieval: A Survey on Recent Research and Development},
  author    = {Cao, Min and Li, Shiping and Li, Juntao and Nie, Liqiang and Zhang, Min},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {5410--5417},
  year      = {2022},
  month     = {7},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2022/759},
  url       = {https://doi.org/10.24963/ijcai.2022/759},
}

@misc{multi,
      title={A Survey on Multi-view Learning}, 
      author={Chang Xu and Dacheng Tao and Chao Xu},
      year={2013},
      eprint={1304.5634},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xu2023multimodal,
      title={Multimodal Learning with Transformers: A Survey}, 
      author={Peng Xu and Xiatian Zhu and David A. Clifton},
      year={2023},
      eprint={2206.06488},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{VQA,
author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
title = {{VQA}: {V}isual {Q}uestion {A}nswering},
booktitle = {International Conference on Computer Vision (ICCV)},
year = {2015},
}

@misc{vinyals2015tell,
      title={Show and Tell: A Neural Image Caption Generator}, 
      author={Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
      year={2015},
      eprint={1411.4555},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{recsys,
author = {Belkin, Nicholas J. and Croft, W. Bruce},
title = {Information Filtering and Information Retrieval: Two Sides of the Same Coin?},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/138859.138861},
doi = {10.1145/138859.138861},
journal = {Commun. ACM},
month = {dec},
pages = {29–38},
numpages = {10},
keywords = {information filtering, information retrieval}
}

@misc{krishna2016visual,
      title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}, 
      author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
      year={2016},
      eprint={1602.07332},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{im2text,
  Author    = {Vicente Ordonez and Girish Kulkarni and Tamara L. Berg},
  Title     = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
  Booktitle = {Neural Information Processing Systems ({NIPS})},
  Year      = {2011},
}

@inproceedings{conceptualcaptions,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@inproceedings{Chun2022ECCVCC,
  title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},
  author={Sanghyuk Chun and Wonjae Kim and Song Park and Minsuk Chang and Seong Joon Oh},
  booktitle={European Conference on Computer Vision},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248006373}
}